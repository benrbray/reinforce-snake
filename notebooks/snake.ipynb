{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9b6fda",
   "metadata": {},
   "source": [
    "# Snake Game\n",
    "\n",
    "This is text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c564873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# standard library\n",
    "import sys;\n",
    "import os;\n",
    "import time;\n",
    "import asyncio;\n",
    "\n",
    "# scientific\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython\n",
    "import ipywidgets\n",
    "\n",
    "# machine learning\n",
    "import gymnasium as gym\n",
    "\n",
    "# add project files to path\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append(os.path.relpath('..'))\n",
    "import gym_snakegame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d8ffb",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a64b05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment passes all checks!\n"
     ]
    }
   ],
   "source": [
    "# register custom snake environment\n",
    "env = gym.make(\"gym_snakegame/SnakeGame-v0\", board_size=8, n_food=1, render_mode='ansi')\n",
    "\n",
    "# check environment validity (optional)\n",
    "# https://gymnasium.farama.org/introduction/create_custom_env/#check-environment-validity\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"Environment passes all checks!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment has issues: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc79facd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ddd22fcb174d8eae0669b707d85706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"gym_snakegame/SnakeGame-v0\", board_size=10, n_food=1, render_mode='ansi')\n",
    "out = ipywidgets.Output(layout={'border': '1px solid black'})\n",
    "IPython.display.display(out)\n",
    "\n",
    "with out:\n",
    "    obs, info = env.reset()\n",
    "    for i in range(10000):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        frame = env.render()\n",
    "        print(frame)\n",
    "        time.sleep(0.08)\n",
    "        IPython.display.clear_output(wait=True)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            obs, info = env.reset()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30724fed",
   "metadata": {},
   "source": [
    "## `SnakeAgent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a6779fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://gymnasium.farama.org/introduction/train_agent/#../tutorials/training_agents\n",
    "\n",
    "import collections\n",
    "\n",
    "class SnakeAgent:\n",
    "  def __init__(\n",
    "      self,\n",
    "      env: gym.Env,\n",
    "      learning_rate: float,\n",
    "      initial_epsilon: float,\n",
    "      epsilon_decay: float,\n",
    "      epsilon_minimum: float,\n",
    "      discount_factor: float = 0.95,\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Initialize a Q-Learning agent.\n",
    "\n",
    "    Args:\n",
    "      `env`:             The training environment\n",
    "      `learning_rate`:   How quickly to update Q-values (0-1)\n",
    "      `initial_epsilon`: Starting exploration rate (usually 1.0)\n",
    "      `epsilon_decay`:   How much to reduce epsilon each episode\n",
    "      `epsilon_minimum`: Minimum exploration rate (usually 0.1)\n",
    "      `discount_factor`: How much to value future rewards (0-1)\n",
    "    \"\"\"\n",
    "\n",
    "    # keep a reference to the training environment\n",
    "    self.env = env;\n",
    "\n",
    "    # learning / exploration rates\n",
    "    self.learning_rate = learning_rate;\n",
    "    self.discount_factor = discount_factor;\n",
    "\n",
    "    self.epsilon = initial_epsilon;\n",
    "    self.epsilon_decay = epsilon_decay;\n",
    "    self.epsilon_minimum = epsilon_minimum;\n",
    "\n",
    "    # the Q-Table maps (state, action) pairs to expected reward\n",
    "    # defaultdict automatically creates entries with zeros for new states\n",
    "    num_actions = env.action_space.n;\n",
    "    self.q_values = collections.defaultdict(lambda: np.zeros(num_actions));\n",
    "\n",
    "    # track learning progress\n",
    "    self.training_error = []\n",
    "  \n",
    "  def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "    \"\"\"\n",
    "    Choose an action using an epsilon-greedy strategy.\n",
    "\n",
    "    Returns:\n",
    "      `action`: Left / Right / Up / Down\n",
    "    \"\"\"\n",
    "\n",
    "    if np.random.random() < self.epsilon:\n",
    "      # EXPLORE, with probability epsilon\n",
    "      return self.env.action_space.sample()\n",
    "    else:\n",
    "      # otherwise EXPLOIT!\n",
    "      return int(np.argmax(self.qvalue(obs)))\n",
    "    \n",
    "  def _get_hash(self, obs):\n",
    "    # to use an (obs, action) pair as the index to a dict,\n",
    "    # we must store them as a hashable data type\n",
    "    vision = tuple(obs[\"vision\"].flatten().tolist());\n",
    "    smell  = tuple(obs[\"smell\"].flatten().tolist());\n",
    "    obs_key = frozenset([vision, smell]);\n",
    "    return obs_key;\n",
    "    \n",
    "  def qvalue(self, obs):\n",
    "    obs_key = self._get_hash(obs);\n",
    "    return self.q_values[obs_key]\n",
    "  \n",
    "  def update(\n",
    "    self,\n",
    "    obs: tuple[int, int, bool],\n",
    "    action: int,\n",
    "    reward: float,\n",
    "    terminated: bool,\n",
    "    next_obs: tuple[int, int, bool] # what is next_obs?\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Update Q-value based on experience.\n",
    "\n",
    "    Args\n",
    "      (`obs`, `action`): The current state and chosen action.\n",
    "      `reward`: The reward received after taking the `action`.\n",
    "      `terminated`: Whether the action caused termination.\n",
    "      `next_obs`: Next observation after taking `action`.\n",
    "    \"\"\"\n",
    "\n",
    "    # estimate our best expected reward from the next state\n",
    "    if terminated:\n",
    "      # no future rewards possible if we're terminated!\n",
    "      future_q_value = 0;\n",
    "    else:\n",
    "      # look for the maximum possible reward from this state according to q-function\n",
    "      future_q_value = np.max(self.qvalue(next_obs))\n",
    "\n",
    "    target_q_value = reward + self.discount_factor * future_q_value;\n",
    "\n",
    "    # temporal difference\n",
    "    temporal_difference = target_q_value - self.qvalue(obs)[action]\n",
    "\n",
    "    # update our estimate in the direction of the error\n",
    "    # learning rate controls step size\n",
    "    self.qvalue(obs)[action] = (\n",
    "      self.qvalue(obs)[action] + self.learning_rate * temporal_difference\n",
    "    )\n",
    "\n",
    "    # track learning progress (useful for debugging)\n",
    "    self.training_error.append(temporal_difference);\n",
    "  \n",
    "  def decay_epsilon(self):\n",
    "    \"\"\"Reduce exploration rate after each episode.\"\"\"\n",
    "    self.epsilon = max(self.epsilon_minimum, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5adf6c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [04:42<00:00, 354.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "learning_rate = 0.01        # How fast to learn (higher = faster but less stable)\n",
    "n_episodes = 100_000        # Number of hands to practice\n",
    "start_epsilon = 1.0         # Start with 100% random actions\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # Reduce exploration over time\n",
    "epsilon_minimum = 0.1         # Always keep some exploration\n",
    "\n",
    "# Create environment and agent\n",
    "env = gym.make(\"gym_snakegame/SnakeGame-v0\", board_size=8, n_food=1, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
    "\n",
    "agent = SnakeAgent(\n",
    "    env=env,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    epsilon_minimum=epsilon_minimum,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    # Start a new hand\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # Play one complete hand\n",
    "    while not done:\n",
    "        # Agent chooses action (initially random, gradually more intelligent)\n",
    "        action = agent.get_action(obs)\n",
    "\n",
    "        # Take action and observe result\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Learn from this experience\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # Move to next state\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    # Reduce exploration rate (agent becomes less random over time)\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb4a255",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e512056b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345c928609e64a4eb18da03c98d03f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"gym_snakegame/SnakeGame-v0\", board_size=10, n_food=3, render_mode='ansi')\n",
    "out = ipywidgets.Output(layout={'border': '1px solid black'})\n",
    "IPython.display.display(out)\n",
    "\n",
    "with out:\n",
    "    obs, info = env.reset()\n",
    "    for i in range(10000):\n",
    "        action = agent.get_action(obs);\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        frame = env.render()\n",
    "        print(frame)\n",
    "        time.sleep(0.08)\n",
    "        IPython.display.clear_output(wait=True)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            obs, info = env.reset()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8607f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "\n",
    "# env = gym.make(\"gym_snakegame/SnakeGame-v0\", board_size=10, n_channel=1, n_target=1, render_mode='rgb_array')\n",
    "\n",
    "# fig,ax = plt.subplots(1,1)\n",
    "# hdisplay = IPython.display.display(\"\", display_id=True)\n",
    "\n",
    "# obs, info = env.reset()\n",
    "# for i in range(10000):\n",
    "#     time.sleep(0.01)\n",
    "\n",
    "#     action = env.action_space.sample()\n",
    "#     obs, reward, terminated, truncated, info = env.step(action)\n",
    "#     frame = env.render()\n",
    "\n",
    "#     ax.imshow(frame);\n",
    "#     hdisplay.update(fig);\n",
    "\n",
    "#     if terminated or truncated:\n",
    "#         obs, info = env.reset()\n",
    "\n",
    "# env.close()\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6b7b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "\n",
    "# from IPython import display\n",
    "\n",
    "# # https://stackoverflow.com/a/65400882\n",
    "# def pltsin(ax, *,hdisplay, colors=['b']):\n",
    "#     x = np.linspace(0,1,100)\n",
    "#     if ax.lines:\n",
    "#         for line in ax.lines:\n",
    "#             line.set_xdata(x)\n",
    "#             y = np.random.random(size=(100,1))\n",
    "#             line.set_ydata(y)\n",
    "#     else:\n",
    "#         for color in colors:\n",
    "#             y = np.random.random(size=(100,1))\n",
    "#             ax.plot(x, y, color)\n",
    "#     hdisplay.update(fig)\n",
    "\n",
    "\n",
    "# fig,ax = plt.subplots(1,1)\n",
    "# hdisplay = IPython.display.display(\"\", display_id=True)\n",
    "\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_xlim(0,1)\n",
    "# ax.set_ylim(0,1)\n",
    "# for f in range(5):\n",
    "#     pltsin(ax, colors=['b', 'r'], hdisplay=hdisplay)\n",
    "#     time.sleep(1)\n",
    "    \n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7209c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import animation\n",
    "# from IPython.display import HTML\n",
    "\n",
    "# # (Your code to create the figure, axes, and initial plot objects)\n",
    "# # ...\n",
    "\n",
    "# # Define the animation function that updates the plot for each frame\n",
    "# def animate(i):\n",
    "#     # Update plot objects (e.g., line data, text) based on frame 'i'\n",
    "#     # ...\n",
    "#     return (line1, line2, txt_title) # Return the objects that were modified\n",
    "\n",
    "# # Create the animation object\n",
    "# anim = animation.FuncAnimation(\n",
    "#     fig, animate, frames=100, interval=20, blit=True\n",
    "# )\n",
    "\n",
    "# # Display the animation as an HTML5 video\n",
    "# HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "281ef099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.animation as animation\n",
    "# import numpy as np\n",
    "# from IPython.display import HTML\n",
    "# plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "# plt.ioff() #needed so the second time you run it you get only single plot\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# x = np.arange(0, 2*np.pi, 0.1)\n",
    "# line, = ax.plot(x, np.sin(x))\n",
    "# z = x.size\n",
    "\n",
    "# def animate(i):\n",
    "#     line.set_ydata(np.sin(x - 2*np.pi*i / z)) \n",
    "#     return line,\n",
    "\n",
    "# ani = animation.FuncAnimation(\n",
    "#     fig, animate,\n",
    "#     frames = z,\n",
    "#     blit=True)\n",
    "# ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e99c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
