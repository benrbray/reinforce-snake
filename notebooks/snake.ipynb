{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9b6fda",
   "metadata": {},
   "source": [
    "# Snake Game\n",
    "\n",
    "This is text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c564873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/projects/ml/snake/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "# standard library\n",
    "import sys;\n",
    "import os;\n",
    "import time;\n",
    "import asyncio;\n",
    "\n",
    "# scientific\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython\n",
    "import ipywidgets\n",
    "\n",
    "# machine learning\n",
    "import gymnasium as gym\n",
    "\n",
    "# add project files to path\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append(os.path.relpath('..'))\n",
    "import gym_snakegame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d8ffb",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a64b05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/projects/ml/snake/.venv/lib/python3.12/site-packages/gymnasium/utils/env_checker.py:384: UserWarning: \u001b[33mWARN: The environment (<OrderEnforcing<PassiveEnvChecker<SnakeGameEnv<gym_snakegame/SnakeGame-v0>>>>) is different from the unwrapped version (<SnakeGameEnv<gym_snakegame/SnakeGame-v0>>). This could effect the environment checker as the environment most likely has a wrapper applied to it. We recommend using the raw environment for `check_env` using `env.unwrapped`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment passes all checks!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/projects/ml/snake/.venv/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:297: UserWarning: \u001b[33mWARN: ANSI/ASCII rendering should produce a string, got <class 'NoneType'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# register custom snake environment\n",
    "env = gym.make(\"gym_snakegame/SnakeGame-v0\", board_size=10, n_channel=1, n_target=1, render_mode='human')\n",
    "\n",
    "# check environment validity (optional)\n",
    "# https://gymnasium.farama.org/introduction/create_custom_env/#check-environment-validity\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"Environment passes all checks!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment has issues: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc79facd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d86d1380c54d9a85800df2477d52b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"gym_snakegame/SnakeGame-v0\", board_size=10, n_channel=1, n_target=1, render_mode='ansi')\n",
    "out = ipywidgets.Output(layout={'border': '1px solid black'})\n",
    "display(out)\n",
    "\n",
    "with out:\n",
    "    obs, info = env.reset()\n",
    "    for i in range(10000):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        frame = env.render()\n",
    "        print(frame)\n",
    "        time.sleep(0.08)\n",
    "        IPython.display.clear_output(wait=True)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            obs, info = env.reset()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30724fed",
   "metadata": {},
   "source": [
    "## `SnakeAgent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6779fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://gymnasium.farama.org/introduction/train_agent/#../tutorials/training_agents\n",
    "\n",
    "import collections\n",
    "\n",
    "class SnakeAgent:\n",
    "  def __init__(\n",
    "      self,\n",
    "      env: gym.Env,\n",
    "      learning_rate: float,\n",
    "      initial_epsilon: float,\n",
    "      epsilon_decay: float,\n",
    "      epsilon_minimum: float,\n",
    "      discount_factor: float = 0.95,\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Initialize a Q-Learning agent.\n",
    "\n",
    "    Args:\n",
    "      `env`:             The training environment\n",
    "      `learning_rate`:   How quickly to update Q-values (0-1)\n",
    "      `initial_epsilon`: Starting exploration rate (usually 1.0)\n",
    "      `epsilon_decay`:   How much to reduce epsilon each episode\n",
    "      `epsilon_minimum`: Minimum exploration rate (usually 0.1)\n",
    "      `discount_factor`: How much to value future rewards (0-1)\n",
    "    \"\"\"\n",
    "\n",
    "    # keep a reference to the training environment\n",
    "    self.env = env;\n",
    "\n",
    "    # learning / exploration rates\n",
    "    self.learning_rate = learning_rate;\n",
    "    self.discount_factor = discount_factor;\n",
    "\n",
    "    self.epsilon = initial_epsilon;\n",
    "    self.epsilon_decay = epsilon_decay;\n",
    "    self.epsilon_minimum = epsilon_minimum;\n",
    "\n",
    "    # the Q-Table maps (state, action) pairs to expected reward\n",
    "    # defaultdict automatically creates entries with zeros for new states\n",
    "    num_actions = env.action_space.n;\n",
    "    self.q_values = collections.defaultdict(lambda: np.zeros(num_actions));\n",
    "\n",
    "    # track learning progress\n",
    "    self.training_error = []\n",
    "  \n",
    "  def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "    \"\"\"\n",
    "    Choose an action using an epsilon-greedy strategy.\n",
    "\n",
    "    Returns:\n",
    "      `action`: Left / Right / Up / Down\n",
    "    \"\"\"\n",
    "\n",
    "    if np.random.random() < self.epsilon:\n",
    "      # EXPLORE, with probability epsilon\n",
    "      return self.env.action_space.sample()\n",
    "    else:\n",
    "      # otherwise EXPLOIT!\n",
    "      return int(np.argmax(self.q_values[obs]))\n",
    "  \n",
    "  def update(\n",
    "    self,\n",
    "    obs: tuple[int, int, bool],\n",
    "    action: int,\n",
    "    reward: float,\n",
    "    terminated: bool,\n",
    "    next_obs: tuple[int, int, bool] # what is next_obs?\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Update Q-value based on experience.\n",
    "\n",
    "    Args\n",
    "      (`obs`, `action`): The current state and chosen action.\n",
    "      `reward`: The reward received after taking the `action`.\n",
    "      `terminated`: Whether the action caused termination.\n",
    "      `next_obs`: Next observation after taking `action`.\n",
    "    \"\"\"\n",
    "\n",
    "    # estimate our best expected reward from the next state\n",
    "    if terminated:\n",
    "      # no future rewards possible if we're terminated!\n",
    "      future_q_value = 0;\n",
    "    else:\n",
    "      # look for the maximum possible reward from this state according to q-function\n",
    "      future_q_value = np.max(self.q_values[next_obs])\n",
    "\n",
    "    target_q_value = reward + self.discount_factor * future_q_value;\n",
    "\n",
    "    # temporal difference\n",
    "    temporal_difference = target_q_value - self.q_values[obs][action]\n",
    "\n",
    "    # update our estimate in the direction of the error\n",
    "    # learning rate controls step size\n",
    "    self.q_values[obs][action] = (\n",
    "      self.q_values[obs][action] + self.learning_rate * temporal_difference\n",
    "    )\n",
    "\n",
    "    # track learning progress (useful for debugging)\n",
    "    self.training_error.append(temporal_difference);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5adf6c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  0 37  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  1  0  0]\n",
      "  [ 0  0  3  2  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(next_obs)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Learn from this experience\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Move to next state\u001b[39;00m\n\u001b[32m     41\u001b[39m done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36mSnakeAgent.update\u001b[39m\u001b[34m(self, obs, action, reward, terminated, next_obs)\u001b[39m\n\u001b[32m     82\u001b[39m   future_q_value = \u001b[32m0\u001b[39m;\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m   \u001b[38;5;66;03m# look for the maximum possible reward from this state according to q-function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m   future_q_value = np.max(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     87\u001b[39m target_q_value = reward + \u001b[38;5;28mself\u001b[39m.discount_factor * future_q_value;\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# temporal difference\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "learning_rate = 0.01        # How fast to learn (higher = faster but less stable)\n",
    "n_episodes = 100_000        # Number of hands to practice\n",
    "start_epsilon = 1.0         # Start with 100% random actions\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # Reduce exploration over time\n",
    "epsilon_minimum = 0.1         # Always keep some exploration\n",
    "\n",
    "# Create environment and agent\n",
    "env = gym.make(\"gym_snakegame/SnakeGame-v0\", board_size=6, n_channel=1, n_target=1, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
    "\n",
    "agent = SnakeAgent(\n",
    "    env=env,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    epsilon_minimum=epsilon_minimum,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    # Start a new hand\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # Play one complete hand\n",
    "    while not done:\n",
    "        # Agent chooses action (initially random, gradually more intelligent)\n",
    "        action = agent.get_action(obs)\n",
    "\n",
    "        # Take action and observe result\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        print(next_obs)\n",
    "\n",
    "        # Learn from this experience\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # Move to next state\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    # Reduce exploration rate (agent becomes less random over time)\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb4a255",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8607f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "\n",
    "# env = gym.make(\"gym_snakegame/SnakeGame-v0\", board_size=10, n_channel=1, n_target=1, render_mode='rgb_array')\n",
    "\n",
    "# fig,ax = plt.subplots(1,1)\n",
    "# hdisplay = IPython.display.display(\"\", display_id=True)\n",
    "\n",
    "# obs, info = env.reset()\n",
    "# for i in range(10000):\n",
    "#     time.sleep(0.01)\n",
    "\n",
    "#     action = env.action_space.sample()\n",
    "#     obs, reward, terminated, truncated, info = env.step(action)\n",
    "#     frame = env.render()\n",
    "\n",
    "#     ax.imshow(frame);\n",
    "#     hdisplay.update(fig);\n",
    "\n",
    "#     if terminated or truncated:\n",
    "#         obs, info = env.reset()\n",
    "\n",
    "# env.close()\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6b7b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "\n",
    "# from IPython import display\n",
    "\n",
    "# # https://stackoverflow.com/a/65400882\n",
    "# def pltsin(ax, *,hdisplay, colors=['b']):\n",
    "#     x = np.linspace(0,1,100)\n",
    "#     if ax.lines:\n",
    "#         for line in ax.lines:\n",
    "#             line.set_xdata(x)\n",
    "#             y = np.random.random(size=(100,1))\n",
    "#             line.set_ydata(y)\n",
    "#     else:\n",
    "#         for color in colors:\n",
    "#             y = np.random.random(size=(100,1))\n",
    "#             ax.plot(x, y, color)\n",
    "#     hdisplay.update(fig)\n",
    "\n",
    "\n",
    "# fig,ax = plt.subplots(1,1)\n",
    "# hdisplay = IPython.display.display(\"\", display_id=True)\n",
    "\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_xlim(0,1)\n",
    "# ax.set_ylim(0,1)\n",
    "# for f in range(5):\n",
    "#     pltsin(ax, colors=['b', 'r'], hdisplay=hdisplay)\n",
    "#     time.sleep(1)\n",
    "    \n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7209c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import animation\n",
    "# from IPython.display import HTML\n",
    "\n",
    "# # (Your code to create the figure, axes, and initial plot objects)\n",
    "# # ...\n",
    "\n",
    "# # Define the animation function that updates the plot for each frame\n",
    "# def animate(i):\n",
    "#     # Update plot objects (e.g., line data, text) based on frame 'i'\n",
    "#     # ...\n",
    "#     return (line1, line2, txt_title) # Return the objects that were modified\n",
    "\n",
    "# # Create the animation object\n",
    "# anim = animation.FuncAnimation(\n",
    "#     fig, animate, frames=100, interval=20, blit=True\n",
    "# )\n",
    "\n",
    "# # Display the animation as an HTML5 video\n",
    "# HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "281ef099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.animation as animation\n",
    "# import numpy as np\n",
    "# from IPython.display import HTML\n",
    "# plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "# plt.ioff() #needed so the second time you run it you get only single plot\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# x = np.arange(0, 2*np.pi, 0.1)\n",
    "# line, = ax.plot(x, np.sin(x))\n",
    "# z = x.size\n",
    "\n",
    "# def animate(i):\n",
    "#     line.set_ydata(np.sin(x - 2*np.pi*i / z)) \n",
    "#     return line,\n",
    "\n",
    "# ani = animation.FuncAnimation(\n",
    "#     fig, animate,\n",
    "#     frames = z,\n",
    "#     blit=True)\n",
    "# ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e99c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
